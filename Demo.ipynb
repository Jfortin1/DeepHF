{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prediction Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhangcd/miniconda3/envs/crispr/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from prediction_util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WNT1(NM_005430.3) DNA sequence for example\n",
    "seq = '''GCGGTGCCGCCCGCCGTGGCCGCCTCAGCCCACCAGCCGGGACCGCGAGCCATGCTGTCCGCCGCCCGCC\n",
    "CCCAGGGTTGTTAAAGCCAGACTGCGAACTCTCGCCACTGCCGCCACCGCCGCGTCCCGTCCCACCGTCG\n",
    "CGGGCAACAACCAAAGTCGCCGCAACTGCAGCACAGAGCGGGCAAAGCCAGGCAGGCCATGGGGCTCTGG\n",
    "GCGCTGTTGCCTGGCTGGGTTTCTGCTACGCTGCTGCTGGCGCTGGCCGCTCTGCCCGCAGCCCTGGCTG\n",
    "CCAACAGCAGTGGCCGATGGTGGGGTATTGTGAACGTAGCCTCCTCCACGAACCTGCTTACAGACTCCAA\n",
    "GAGTCTGCAACTGGTACTCGAGCCCAGTCTGCAGCTGTTGAGCCGCAAACAGCGGCGTCTGATACGCCAA\n",
    "AATCCGGGGATCCTGCACAGCGTGAGTGGGGGGCTGCAGAGTGCCGTGCGCGAGTGCAAGTGGCAGTTCC\n",
    "GGAATCGCCGCTGGAACTGTCCCACTGCTCCAGGGCCCCACCTCTTCGGCAAGATCGTCAACCGAGGCTG\n",
    "TCGAGAAACGGCGTTTATCTTCGCTATCACCTCCGCCGGGGTCACCCATTCGGTGGCGCGCTCCTGCTCA\n",
    "GAAGGTTCCATCGAATCCTGCACGTGTGACTACCGGCGGCGCGGCCCCGGGGGCCCCGACTGGCACTGGG\n",
    "GGGGCTGCAGCGACAACATTGACTTCGGCCGCCTCTTCGGCCGGGAGTTCGTGGACTCCGGGGAGAAGGG\n",
    "GCGGGACCTGCGCTTCCTCATGAACCTTCACAACAACGAGGCAGGCCGTACGACCGTATTCTCCGAGATG\n",
    "CGCCAGGAGTGCAAGTGCCACGGGATGTCCGGCTCATGCACGGTGCGCACGTGCTGGATGCGGCTGCCCA\n",
    "CGCTGCGCGCCGTGGGCGATGTGCTGCGCGACCGCTTCGACGGCGCCTCGCGCGTCCTGTACGGCAACCG\n",
    "CGGCAGCAACCGCGCTTCGCGGGCGGAGCTGCTGCGCCTGGAGCCGGAAGACCCGGCCCACAAACCGCCC\n",
    "TCCCCCCACGACCTCGTCTACTTCGAGAAATCGCCCAACTTCTGCACGTACAGCGGACGCCTGGGCACAG\n",
    "CAGGCACGGCAGGGCGCGCCTGTAACAGCTCGTCGCCCGCGCTGGACGGCTGCGAGCTGCTCTGCTGCGG\n",
    "CAGGGGCCACCGCACGCGCACGCAGCGCGTCACCGAGCGCTGCAACTGCACCTTCCACTGGTGCTGCCAC\n",
    "GTCAGCTGCCGCAACTGCACGCACACGCGCGTACTGCACGAGTGTCTGTGAGGCGCTGCGCGGACTCGCC\n",
    "CCCAGGAACGCTCTCCTCGAGCCCTCCCCCAAACAGACTCGCTAGCACTCAAGACCCGGTTATTCGCCCA\n",
    "CCCGAGTACCTCCAGTCACACTCCCCGCGGTTCATACGCATCCCATCTCTCCCACTTCCTCCTACCTGGG\n",
    "GACTCCTCAAACCACTTGCCTGGGGCGGCATGAACCCTCTTGCCATCCTGATGGACCTGCCCCGGACCTA\n",
    "CCTCCCTCCCTCTCCGCGGGAGACCCCTTGTTGCACTGCCCCCTGCTTGGCCAGGAGGTGAGAGAAGGAT\n",
    "GGGTCCCCTCCGCCATGGGGTCGGCTCCTGATGGTGTCATTCTGCCTGCTCCATCGCGCCAGCGACCTCT\n",
    "CTGCCTCTCTTCTTCCCCTTTGTCCTGCGTTTTCTCCGGGTCCTCCTAAGTCCCTTCCTATTCTCCTGCC\n",
    "ATGGGTGCAGACCCTGAACCCACACCTGGGCATCAGGGCCTTTCTCCTCCCCACCTGTAGCTGAAGCAGG\n",
    "AGGTTACAGGGCAAAAGGGCAGCTGTGATGATGTGGAAATGAGGTTGGGGGAACCAGCAGAAATGCCCCC\n",
    "ATTCTCCCAGTCTCTGTCGTGGAGCCATTGAACAGCTGTGAGCCATGCCTCCCTGGGCCACCTCCTACCC\n",
    "CTTCCTGTCCTGCCTCCTCATCAGTGTGTAAATAATTTGCACTGAAACGTGGATACAGAGCCACGAGTTT\n",
    "GGATGTTGTAAATAAAACTATTTATTGTGCTGGGTCCCAGCCTGGTTTGCAAAGACCACCTCCAACCCAA\n",
    "CCCAATCCCTCTCCACTCTTCTCTCCTTTCTCCCTGCAGCCTTTTCTGGTCCCTCTTCTCTCCTCAGTTT\n",
    "CTCAAAGATGCGTTTGCCTCCTGGAATCAGTATTTCCTTCCACTGTAGCTATTAGCGGCTCCTCGCCCCC\n",
    "ACCAGTGTAGCATCTTCCTCTGCAGAATAAAATCTCTATTTTTA'''\n",
    "enzyme = 'esp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing sencondary structure features.\n",
      "N-None,N2-441\n",
      "N-None,N2-441\n",
      "Constructing nucleotide features.\n",
      "N-None,N2-441\n",
      "N-None,N2-441\n"
     ]
    }
   ],
   "source": [
    "result = effciency_predict(seq.replace(os.linesep, ''), enzyme)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constructing sencondary structure features.\n",
      "N-None,N2-1\n",
      "N-None,N2-1\n",
      "Constructing nucleotide features.\n",
      "N-None,N2-1\n",
      "N-None,N2-1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Strand</th>\n",
       "      <th>Cut_Pos</th>\n",
       "      <th>PAM</th>\n",
       "      <th>gRNA_Seq</th>\n",
       "      <th>Efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "      <td>17</td>\n",
       "      <td>CGG</td>\n",
       "      <td>ACGTGTGACTACCGGCGGCG</td>\n",
       "      <td>0.28913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index Strand  Cut_Pos  PAM              gRNA_Seq  Efficiency\n",
       "0      0      +       17  CGG  ACGTGTGACTACCGGCGGCG     0.28913"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "effciency_predict('ACGTGTGACTACCGGCGGCGCGG','esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Strand</th>\n",
       "      <th>Cut_Pos</th>\n",
       "      <th>PAM</th>\n",
       "      <th>gRNA_Seq</th>\n",
       "      <th>Efficiency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>292</th>\n",
       "      <td>292</td>\n",
       "      <td>-</td>\n",
       "      <td>856</td>\n",
       "      <td>GGG</td>\n",
       "      <td>TGGGATGCGTATGAACCGCG</td>\n",
       "      <td>0.73154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>290</td>\n",
       "      <td>-</td>\n",
       "      <td>854</td>\n",
       "      <td>CGG</td>\n",
       "      <td>GATGGGATGCGTATGAACCG</td>\n",
       "      <td>0.68387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>242</td>\n",
       "      <td>-</td>\n",
       "      <td>575</td>\n",
       "      <td>AGG</td>\n",
       "      <td>GGAGGACCCGGAGAAAACGC</td>\n",
       "      <td>0.67997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>311</th>\n",
       "      <td>311</td>\n",
       "      <td>-</td>\n",
       "      <td>950</td>\n",
       "      <td>GGG</td>\n",
       "      <td>CTCGAGGAGAGCGTTCCTGG</td>\n",
       "      <td>0.67690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>176</td>\n",
       "      <td>-</td>\n",
       "      <td>69</td>\n",
       "      <td>TGG</td>\n",
       "      <td>GGAGCCGCTAATAGCTACAG</td>\n",
       "      <td>0.67012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>187</td>\n",
       "      <td>-</td>\n",
       "      <td>166</td>\n",
       "      <td>TGG</td>\n",
       "      <td>AGGGAGAAAGGAGAGAAGAG</td>\n",
       "      <td>0.66913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>247</td>\n",
       "      <td>-</td>\n",
       "      <td>603</td>\n",
       "      <td>AGG</td>\n",
       "      <td>GGGGAAGAAGAGAGGCAGAG</td>\n",
       "      <td>0.66690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>221</td>\n",
       "      <td>-</td>\n",
       "      <td>393</td>\n",
       "      <td>GGG</td>\n",
       "      <td>GACAGAGACTGGGAGAATGG</td>\n",
       "      <td>0.66322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>304</td>\n",
       "      <td>-</td>\n",
       "      <td>923</td>\n",
       "      <td>GGG</td>\n",
       "      <td>GTGCTAGCGAGTCTGTTTGG</td>\n",
       "      <td>0.65476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103</th>\n",
       "      <td>103</td>\n",
       "      <td>+</td>\n",
       "      <td>1128</td>\n",
       "      <td>GGG</td>\n",
       "      <td>TGGGCACAGCAGGCACGGCA</td>\n",
       "      <td>0.64987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349</th>\n",
       "      <td>349</td>\n",
       "      <td>-</td>\n",
       "      <td>1421</td>\n",
       "      <td>TGG</td>\n",
       "      <td>TGCATGAGCCGGACATCCCG</td>\n",
       "      <td>0.64846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>225</td>\n",
       "      <td>-</td>\n",
       "      <td>479</td>\n",
       "      <td>GGG</td>\n",
       "      <td>CTCCTGCTTCAGCTACAGGT</td>\n",
       "      <td>0.64492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>423</th>\n",
       "      <td>423</td>\n",
       "      <td>-</td>\n",
       "      <td>2209</td>\n",
       "      <td>GGG</td>\n",
       "      <td>GTCTGGCTTTAACAACCCTG</td>\n",
       "      <td>0.64413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>42</td>\n",
       "      <td>+</td>\n",
       "      <td>551</td>\n",
       "      <td>AGG</td>\n",
       "      <td>TTCGGCAAGATCGTCAACCG</td>\n",
       "      <td>0.64072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>237</td>\n",
       "      <td>-</td>\n",
       "      <td>546</td>\n",
       "      <td>AGG</td>\n",
       "      <td>CCCATGGCAGGAGAATAGGA</td>\n",
       "      <td>0.63614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>181</td>\n",
       "      <td>-</td>\n",
       "      <td>127</td>\n",
       "      <td>AGG</td>\n",
       "      <td>TGAGAAACTGAGGAGAGAAG</td>\n",
       "      <td>0.62194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>62</td>\n",
       "      <td>+</td>\n",
       "      <td>697</td>\n",
       "      <td>GGG</td>\n",
       "      <td>GGGCCCCGACTGGCACTGGG</td>\n",
       "      <td>0.61947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>188</td>\n",
       "      <td>-</td>\n",
       "      <td>171</td>\n",
       "      <td>AGG</td>\n",
       "      <td>GAAAGGAGAGAAGAGTGGAG</td>\n",
       "      <td>0.60351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>421</td>\n",
       "      <td>-</td>\n",
       "      <td>2207</td>\n",
       "      <td>TGG</td>\n",
       "      <td>CAGTCTGGCTTTAACAACCC</td>\n",
       "      <td>0.60268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>+</td>\n",
       "      <td>1184</td>\n",
       "      <td>CGG</td>\n",
       "      <td>GGCTGCGAGCTGCTCTGCTG</td>\n",
       "      <td>0.60202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>+</td>\n",
       "      <td>1465</td>\n",
       "      <td>GGG</td>\n",
       "      <td>CTCCCACTTCCTCCTACCTG</td>\n",
       "      <td>0.60045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>+</td>\n",
       "      <td>1424</td>\n",
       "      <td>CGG</td>\n",
       "      <td>ACCTCCAGTCACACTCCCCG</td>\n",
       "      <td>0.59730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>345</td>\n",
       "      <td>-</td>\n",
       "      <td>1359</td>\n",
       "      <td>CGG</td>\n",
       "      <td>GTCGCGCAGCACATCGCCCA</td>\n",
       "      <td>0.58892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>132</td>\n",
       "      <td>+</td>\n",
       "      <td>1607</td>\n",
       "      <td>GGG</td>\n",
       "      <td>CCAGGAGGTGAGAGAAGGAT</td>\n",
       "      <td>0.58863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>+</td>\n",
       "      <td>446</td>\n",
       "      <td>GGG</td>\n",
       "      <td>ATCCTGCACAGCGTGAGTGG</td>\n",
       "      <td>0.58802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>396</td>\n",
       "      <td>-</td>\n",
       "      <td>1956</td>\n",
       "      <td>AGG</td>\n",
       "      <td>GTCTGTAAGCAGGTTCGTGG</td>\n",
       "      <td>0.58644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>264</td>\n",
       "      <td>-</td>\n",
       "      <td>715</td>\n",
       "      <td>GGG</td>\n",
       "      <td>GCAGGGGGCAGTGCAACAAG</td>\n",
       "      <td>0.58443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>260</td>\n",
       "      <td>-</td>\n",
       "      <td>699</td>\n",
       "      <td>GGG</td>\n",
       "      <td>CTCACCTCCTGGCCAAGCAG</td>\n",
       "      <td>0.57722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>329</td>\n",
       "      <td>-</td>\n",
       "      <td>1226</td>\n",
       "      <td>GGG</td>\n",
       "      <td>CGAAGTAGACGAGGTCGTGG</td>\n",
       "      <td>0.57161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>171</td>\n",
       "      <td>-</td>\n",
       "      <td>40</td>\n",
       "      <td>TGG</td>\n",
       "      <td>AGAGGAAGATGCTACACTGG</td>\n",
       "      <td>0.56234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>+</td>\n",
       "      <td>1492</td>\n",
       "      <td>CGG</td>\n",
       "      <td>CTCAAACCACTTGCCTGGGG</td>\n",
       "      <td>0.09007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>351</th>\n",
       "      <td>351</td>\n",
       "      <td>-</td>\n",
       "      <td>1446</td>\n",
       "      <td>CGG</td>\n",
       "      <td>CTTGCACTCCTGGCGCATCT</td>\n",
       "      <td>0.08765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>413</td>\n",
       "      <td>-</td>\n",
       "      <td>2152</td>\n",
       "      <td>CGG</td>\n",
       "      <td>TGTTGCCCGCGACGGTGGGA</td>\n",
       "      <td>0.08654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>+</td>\n",
       "      <td>223</td>\n",
       "      <td>GGG</td>\n",
       "      <td>CTGGGCGCTGTTGCCTGGCT</td>\n",
       "      <td>0.08498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>167</td>\n",
       "      <td>+</td>\n",
       "      <td>2188</td>\n",
       "      <td>TGG</td>\n",
       "      <td>TCAAAGATGCGTTTGCCTCC</td>\n",
       "      <td>0.08429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293</th>\n",
       "      <td>293</td>\n",
       "      <td>-</td>\n",
       "      <td>867</td>\n",
       "      <td>TGG</td>\n",
       "      <td>TGAACCGCGGGGAGTGTGAC</td>\n",
       "      <td>0.08390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>201</td>\n",
       "      <td>-</td>\n",
       "      <td>303</td>\n",
       "      <td>AGG</td>\n",
       "      <td>AAATTATTTACACACTGATG</td>\n",
       "      <td>0.08368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>216</td>\n",
       "      <td>-</td>\n",
       "      <td>382</td>\n",
       "      <td>TGG</td>\n",
       "      <td>AATGGCTCCACGACAGAGAC</td>\n",
       "      <td>0.08337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>+</td>\n",
       "      <td>2057</td>\n",
       "      <td>TGG</td>\n",
       "      <td>AATAAAACTATTTATTGTGC</td>\n",
       "      <td>0.08277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>+</td>\n",
       "      <td>271</td>\n",
       "      <td>TGG</td>\n",
       "      <td>GGCCGCTCTGCCCGCAGCCC</td>\n",
       "      <td>0.08127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>199</td>\n",
       "      <td>-</td>\n",
       "      <td>213</td>\n",
       "      <td>GGG</td>\n",
       "      <td>TGGTCTTTGCAAACCAGGCT</td>\n",
       "      <td>0.08125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>233</td>\n",
       "      <td>-</td>\n",
       "      <td>517</td>\n",
       "      <td>GGG</td>\n",
       "      <td>GATGCCCAGGTGTGGGTTCA</td>\n",
       "      <td>0.08122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>400</td>\n",
       "      <td>-</td>\n",
       "      <td>2006</td>\n",
       "      <td>AGG</td>\n",
       "      <td>GGCCACTGCTGTTGGCAGCC</td>\n",
       "      <td>0.08095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>57</td>\n",
       "      <td>+</td>\n",
       "      <td>687</td>\n",
       "      <td>TGG</td>\n",
       "      <td>GCGGCCCCGGGGGCCCCGAC</td>\n",
       "      <td>0.08050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>342</td>\n",
       "      <td>-</td>\n",
       "      <td>1313</td>\n",
       "      <td>AGG</td>\n",
       "      <td>TGCTGCCGCGGTTGCCGTAC</td>\n",
       "      <td>0.08020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>+</td>\n",
       "      <td>250</td>\n",
       "      <td>TGG</td>\n",
       "      <td>TGCTACGCTGCTGCTGGCGC</td>\n",
       "      <td>0.07936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>381</td>\n",
       "      <td>-</td>\n",
       "      <td>1759</td>\n",
       "      <td>TGG</td>\n",
       "      <td>TTGCCGAAGAGGTGGGGCCC</td>\n",
       "      <td>0.07813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>65</td>\n",
       "      <td>+</td>\n",
       "      <td>734</td>\n",
       "      <td>CGG</td>\n",
       "      <td>ATTGACTTCGGCCGCCTCTT</td>\n",
       "      <td>0.07631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>385</td>\n",
       "      <td>-</td>\n",
       "      <td>1790</td>\n",
       "      <td>CGG</td>\n",
       "      <td>GACAGTTCCAGCGGCGATTC</td>\n",
       "      <td>0.07492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>+</td>\n",
       "      <td>1188</td>\n",
       "      <td>AGG</td>\n",
       "      <td>GCGAGCTGCTCTGCTGCGGC</td>\n",
       "      <td>0.07404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>232</td>\n",
       "      <td>-</td>\n",
       "      <td>516</td>\n",
       "      <td>AGG</td>\n",
       "      <td>TGATGCCCAGGTGTGGGTTC</td>\n",
       "      <td>0.07324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>186</td>\n",
       "      <td>-</td>\n",
       "      <td>154</td>\n",
       "      <td>AGG</td>\n",
       "      <td>AGAAAAGGCTGCAGGGAGAA</td>\n",
       "      <td>0.07246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>414</td>\n",
       "      <td>-</td>\n",
       "      <td>2153</td>\n",
       "      <td>GGG</td>\n",
       "      <td>GTTGCCCGCGACGGTGGGAC</td>\n",
       "      <td>0.07096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>223</td>\n",
       "      <td>-</td>\n",
       "      <td>475</td>\n",
       "      <td>AGG</td>\n",
       "      <td>TAACCTCCTGCTTCAGCTAC</td>\n",
       "      <td>0.06838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>317</td>\n",
       "      <td>-</td>\n",
       "      <td>1079</td>\n",
       "      <td>CGG</td>\n",
       "      <td>TGACGCGCTGCGTGCGCGTG</td>\n",
       "      <td>0.06663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>139</td>\n",
       "      <td>+</td>\n",
       "      <td>1714</td>\n",
       "      <td>GGG</td>\n",
       "      <td>CTTTGTCCTGCGTTTTCTCC</td>\n",
       "      <td>0.06236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>165</td>\n",
       "      <td>+</td>\n",
       "      <td>2069</td>\n",
       "      <td>TGG</td>\n",
       "      <td>TATTGTGCTGGGTCCCAGCC</td>\n",
       "      <td>0.06098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>166</td>\n",
       "      <td>+</td>\n",
       "      <td>2143</td>\n",
       "      <td>TGG</td>\n",
       "      <td>TTTCTCCCTGCAGCCTTTTC</td>\n",
       "      <td>0.05567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>409</th>\n",
       "      <td>409</td>\n",
       "      <td>-</td>\n",
       "      <td>2128</td>\n",
       "      <td>TGG</td>\n",
       "      <td>TGCTGCAGTTGCGGCGACTT</td>\n",
       "      <td>0.05504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>164</td>\n",
       "      <td>+</td>\n",
       "      <td>2058</td>\n",
       "      <td>GGG</td>\n",
       "      <td>ATAAAACTATTTATTGTGCT</td>\n",
       "      <td>0.04642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>441 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index Strand  Cut_Pos  PAM              gRNA_Seq  Efficiency\n",
       "292    292      -      856  GGG  TGGGATGCGTATGAACCGCG     0.73154\n",
       "290    290      -      854  CGG  GATGGGATGCGTATGAACCG     0.68387\n",
       "242    242      -      575  AGG  GGAGGACCCGGAGAAAACGC     0.67997\n",
       "311    311      -      950  GGG  CTCGAGGAGAGCGTTCCTGG     0.67690\n",
       "176    176      -       69  TGG  GGAGCCGCTAATAGCTACAG     0.67012\n",
       "187    187      -      166  TGG  AGGGAGAAAGGAGAGAAGAG     0.66913\n",
       "247    247      -      603  AGG  GGGGAAGAAGAGAGGCAGAG     0.66690\n",
       "221    221      -      393  GGG  GACAGAGACTGGGAGAATGG     0.66322\n",
       "304    304      -      923  GGG  GTGCTAGCGAGTCTGTTTGG     0.65476\n",
       "103    103      +     1128  GGG  TGGGCACAGCAGGCACGGCA     0.64987\n",
       "349    349      -     1421  TGG  TGCATGAGCCGGACATCCCG     0.64846\n",
       "225    225      -      479  GGG  CTCCTGCTTCAGCTACAGGT     0.64492\n",
       "423    423      -     2209  GGG  GTCTGGCTTTAACAACCCTG     0.64413\n",
       "42      42      +      551  AGG  TTCGGCAAGATCGTCAACCG     0.64072\n",
       "237    237      -      546  AGG  CCCATGGCAGGAGAATAGGA     0.63614\n",
       "181    181      -      127  AGG  TGAGAAACTGAGGAGAGAAG     0.62194\n",
       "62      62      +      697  GGG  GGGCCCCGACTGGCACTGGG     0.61947\n",
       "188    188      -      171  AGG  GAAAGGAGAGAAGAGTGGAG     0.60351\n",
       "421    421      -     2207  TGG  CAGTCTGGCTTTAACAACCC     0.60268\n",
       "106    106      +     1184  CGG  GGCTGCGAGCTGCTCTGCTG     0.60202\n",
       "118    118      +     1465  GGG  CTCCCACTTCCTCCTACCTG     0.60045\n",
       "115    115      +     1424  CGG  ACCTCCAGTCACACTCCCCG     0.59730\n",
       "345    345      -     1359  CGG  GTCGCGCAGCACATCGCCCA     0.58892\n",
       "132    132      +     1607  GGG  CCAGGAGGTGAGAGAAGGAT     0.58863\n",
       "34      34      +      446  GGG  ATCCTGCACAGCGTGAGTGG     0.58802\n",
       "396    396      -     1956  AGG  GTCTGTAAGCAGGTTCGTGG     0.58644\n",
       "264    264      -      715  GGG  GCAGGGGGCAGTGCAACAAG     0.58443\n",
       "260    260      -      699  GGG  CTCACCTCCTGGCCAAGCAG     0.57722\n",
       "329    329      -     1226  GGG  CGAAGTAGACGAGGTCGTGG     0.57161\n",
       "171    171      -       40  TGG  AGAGGAAGATGCTACACTGG     0.56234\n",
       "..     ...    ...      ...  ...                   ...         ...\n",
       "122    122      +     1492  CGG  CTCAAACCACTTGCCTGGGG     0.09007\n",
       "351    351      -     1446  CGG  CTTGCACTCCTGGCGCATCT     0.08765\n",
       "413    413      -     2152  CGG  TGTTGCCCGCGACGGTGGGA     0.08654\n",
       "17      17      +      223  GGG  CTGGGCGCTGTTGCCTGGCT     0.08498\n",
       "167    167      +     2188  TGG  TCAAAGATGCGTTTGCCTCC     0.08429\n",
       "293    293      -      867  TGG  TGAACCGCGGGGAGTGTGAC     0.08390\n",
       "201    201      -      303  AGG  AAATTATTTACACACTGATG     0.08368\n",
       "216    216      -      382  TGG  AATGGCTCCACGACAGAGAC     0.08337\n",
       "163    163      +     2057  TGG  AATAAAACTATTTATTGTGC     0.08277\n",
       "20      20      +      271  TGG  GGCCGCTCTGCCCGCAGCCC     0.08127\n",
       "199    199      -      213  GGG  TGGTCTTTGCAAACCAGGCT     0.08125\n",
       "233    233      -      517  GGG  GATGCCCAGGTGTGGGTTCA     0.08122\n",
       "400    400      -     2006  AGG  GGCCACTGCTGTTGGCAGCC     0.08095\n",
       "57      57      +      687  TGG  GCGGCCCCGGGGGCCCCGAC     0.08050\n",
       "342    342      -     1313  AGG  TGCTGCCGCGGTTGCCGTAC     0.08020\n",
       "19      19      +      250  TGG  TGCTACGCTGCTGCTGGCGC     0.07936\n",
       "381    381      -     1759  TGG  TTGCCGAAGAGGTGGGGCCC     0.07813\n",
       "65      65      +      734  CGG  ATTGACTTCGGCCGCCTCTT     0.07631\n",
       "385    385      -     1790  CGG  GACAGTTCCAGCGGCGATTC     0.07492\n",
       "107    107      +     1188  AGG  GCGAGCTGCTCTGCTGCGGC     0.07404\n",
       "232    232      -      516  AGG  TGATGCCCAGGTGTGGGTTC     0.07324\n",
       "186    186      -      154  AGG  AGAAAAGGCTGCAGGGAGAA     0.07246\n",
       "414    414      -     2153  GGG  GTTGCCCGCGACGGTGGGAC     0.07096\n",
       "223    223      -      475  AGG  TAACCTCCTGCTTCAGCTAC     0.06838\n",
       "317    317      -     1079  CGG  TGACGCGCTGCGTGCGCGTG     0.06663\n",
       "139    139      +     1714  GGG  CTTTGTCCTGCGTTTTCTCC     0.06236\n",
       "165    165      +     2069  TGG  TATTGTGCTGGGTCCCAGCC     0.06098\n",
       "166    166      +     2143  TGG  TTTCTCCCTGCAGCCTTTTC     0.05567\n",
       "409    409      -     2128  TGG  TGCTGCAGTTGCGGCGACTT     0.05504\n",
       "164    164      +     2058  GGG  ATAAAACTATTTATTGTGCT     0.04642\n",
       "\n",
       "[441 rows x 6 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Prediction metrics demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(X,X_biofeat,y, test_size = 0.15,random_state=40):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "       X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    X_train_biofeat, X_test_biofeat, y_train, y_test = train_test_split(\n",
    "       X_biofeat, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, X_train_biofeat, X_test_biofeat, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import  mean_squared_error, r2_score\n",
    "def get_metrics(model,model_type='esp'):\n",
    "    if model_type == 'esp':\n",
    "        X,X_biofeat,y = esp_data\n",
    "    elif model_type == 'hf':\n",
    "        X,X_biofeat,y = hf_data\n",
    "    X_train, X_test, X_train_biofeat, X_test_biofeat, y_train, y_test = load_data(X, X_biofeat, y,random_state=40) \n",
    "    y_train_pred = model.predict([X_train,X_train_biofeat])\n",
    "    y_test_pred = model.predict([X_test,X_test_biofeat])\n",
    "    mse = mean_squared_error( y_test, y_test_pred)\n",
    "    spearmanr = sp.stats.spearmanr(y_test, y_test_pred)[0]    \n",
    "    return 'MES:' + str(mse),'Spearman:' + str(spearmanr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.realpath( './' )\n",
    "esp_model_file_path = os.path.join( dir_path, 'models/esp_rnn_model.hd5' )\n",
    "hf_model_file_path = os.path.join( dir_path, 'models/hf_rnn_model.hd5' )\n",
    "model_esp = load_model( esp_model_file_path)\n",
    "model_hf = load_model( hf_model_file_path )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "with open('data/esp_seq_data_array.pkl', 'rb') as handle:\n",
    "    esp_data = pickle.load(handle)\n",
    "with open('data/hf_seq_data_array.pkl', 'rb') as handle:\n",
    "    hf_data = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MES:0.007770613899385625', 'Spearman:0.8861134817457584')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## esp model prediction metrics\n",
    "get_metrics(model_esp,'esp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MES:0.008558173115465145', 'Spearman:0.8805147791848338')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## hf model prediction metrics\n",
    "get_metrics(model_hf,'hf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Model training demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 44841 samples, validate on 4983 samples\n",
      "Epoch 1/45\n",
      " - 26s - loss: 0.2258 - val_loss: 0.0343\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.03432, storing weights.\n",
      "Epoch 2/45\n",
      " - 24s - loss: 0.0412 - val_loss: 0.0341\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.03432 to 0.03406, storing weights.\n",
      "Epoch 3/45\n",
      " - 24s - loss: 0.0381 - val_loss: 0.0334\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.03406 to 0.03336, storing weights.\n",
      "Epoch 4/45\n",
      " - 24s - loss: 0.0356 - val_loss: 0.0309\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03336 to 0.03091, storing weights.\n",
      "Epoch 5/45\n",
      " - 24s - loss: 0.0325 - val_loss: 0.0288\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.03091 to 0.02877, storing weights.\n",
      "Epoch 6/45\n",
      " - 23s - loss: 0.0288 - val_loss: 0.0242\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.02877 to 0.02424, storing weights.\n",
      "Epoch 7/45\n",
      " - 23s - loss: 0.0259 - val_loss: 0.0228\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.02424 to 0.02284, storing weights.\n",
      "Epoch 8/45\n",
      " - 23s - loss: 0.0239 - val_loss: 0.0222\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.02284 to 0.02217, storing weights.\n",
      "Epoch 9/45\n",
      " - 23s - loss: 0.0225 - val_loss: 0.0199\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.02217 to 0.01988, storing weights.\n",
      "Epoch 10/45\n",
      " - 24s - loss: 0.0214 - val_loss: 0.0195\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01988 to 0.01951, storing weights.\n",
      "Epoch 11/45\n",
      " - 23s - loss: 0.0202 - val_loss: 0.0178\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01951 to 0.01775, storing weights.\n",
      "Epoch 12/45\n",
      " - 23s - loss: 0.0192 - val_loss: 0.0165\n",
      "\n",
      "Epoch 00012: val_loss improved from 0.01775 to 0.01650, storing weights.\n",
      "Epoch 13/45\n",
      " - 24s - loss: 0.0186 - val_loss: 0.0171\n",
      "\n",
      "Epoch 00013: val_loss did not improve.\n",
      "Epoch 14/45\n",
      " - 23s - loss: 0.0179 - val_loss: 0.0156\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.01650 to 0.01557, storing weights.\n",
      "Epoch 15/45\n",
      " - 24s - loss: 0.0172 - val_loss: 0.0149\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01557 to 0.01492, storing weights.\n",
      "Epoch 16/45\n",
      " - 23s - loss: 0.0166 - val_loss: 0.0145\n",
      "\n",
      "Epoch 00016: val_loss improved from 0.01492 to 0.01453, storing weights.\n",
      "Epoch 17/45\n",
      " - 24s - loss: 0.0160 - val_loss: 0.0135\n",
      "\n",
      "Epoch 00017: val_loss improved from 0.01453 to 0.01351, storing weights.\n",
      "Epoch 18/45\n",
      " - 23s - loss: 0.0155 - val_loss: 0.0134\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.01351 to 0.01343, storing weights.\n",
      "Epoch 19/45\n",
      " - 24s - loss: 0.0150 - val_loss: 0.0130\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.01343 to 0.01298, storing weights.\n",
      "Epoch 20/45\n",
      " - 23s - loss: 0.0145 - val_loss: 0.0125\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01298 to 0.01245, storing weights.\n",
      "Epoch 21/45\n",
      " - 24s - loss: 0.0139 - val_loss: 0.0120\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.01245 to 0.01197, storing weights.\n",
      "Epoch 22/45\n",
      " - 23s - loss: 0.0136 - val_loss: 0.0116\n",
      "\n",
      "Epoch 00022: val_loss improved from 0.01197 to 0.01157, storing weights.\n",
      "Epoch 23/45\n",
      " - 25s - loss: 0.0132 - val_loss: 0.0115\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01157 to 0.01147, storing weights.\n",
      "Epoch 24/45\n",
      " - 26s - loss: 0.0129 - val_loss: 0.0114\n",
      "\n",
      "Epoch 00024: val_loss improved from 0.01147 to 0.01141, storing weights.\n",
      "Epoch 25/45\n",
      " - 25s - loss: 0.0126 - val_loss: 0.0111\n",
      "\n",
      "Epoch 00025: val_loss improved from 0.01141 to 0.01111, storing weights.\n",
      "Epoch 26/45\n",
      " - 25s - loss: 0.0122 - val_loss: 0.0107\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.01111 to 0.01074, storing weights.\n",
      "Epoch 27/45\n",
      " - 27s - loss: 0.0119 - val_loss: 0.0106\n",
      "\n",
      "Epoch 00027: val_loss improved from 0.01074 to 0.01058, storing weights.\n",
      "Epoch 28/45\n",
      " - 24s - loss: 0.0117 - val_loss: 0.0104\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.01058 to 0.01044, storing weights.\n",
      "Epoch 29/45\n",
      " - 25s - loss: 0.0116 - val_loss: 0.0105\n",
      "\n",
      "Epoch 00029: val_loss did not improve.\n",
      "Epoch 30/45\n",
      " - 25s - loss: 0.0114 - val_loss: 0.0103\n",
      "\n",
      "Epoch 00030: val_loss improved from 0.01044 to 0.01027, storing weights.\n",
      "Epoch 31/45\n",
      " - 27s - loss: 0.0112 - val_loss: 0.0102\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.01027 to 0.01019, storing weights.\n",
      "Epoch 32/45\n",
      " - 28s - loss: 0.0110 - val_loss: 0.0100\n",
      "\n",
      "Epoch 00032: val_loss improved from 0.01019 to 0.00998, storing weights.\n",
      "Epoch 33/45\n",
      " - 26s - loss: 0.0108 - val_loss: 0.0100\n",
      "\n",
      "Epoch 00033: val_loss did not improve.\n",
      "Epoch 34/45\n",
      " - 28s - loss: 0.0106 - val_loss: 0.0098\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.00998 to 0.00983, storing weights.\n",
      "Epoch 35/45\n",
      " - 25s - loss: 0.0105 - val_loss: 0.0098\n",
      "\n",
      "Epoch 00035: val_loss improved from 0.00983 to 0.00978, storing weights.\n",
      "Epoch 36/45\n",
      " - 25s - loss: 0.0104 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00978 to 0.00968, storing weights.\n",
      "Epoch 37/45\n",
      " - 25s - loss: 0.0102 - val_loss: 0.0096\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00968 to 0.00964, storing weights.\n",
      "Epoch 38/45\n",
      " - 26s - loss: 0.0101 - val_loss: 0.0097\n",
      "\n",
      "Epoch 00038: val_loss did not improve.\n",
      "Epoch 39/45\n",
      " - 27s - loss: 0.0099 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.00964 to 0.00952, storing weights.\n",
      "Epoch 40/45\n",
      " - 25s - loss: 0.0099 - val_loss: 0.0096\n",
      "\n",
      "Epoch 00040: val_loss did not improve.\n",
      "Epoch 41/45\n",
      " - 26s - loss: 0.0097 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00041: val_loss improved from 0.00952 to 0.00945, storing weights.\n",
      "Epoch 42/45\n",
      " - 25s - loss: 0.0095 - val_loss: 0.0095\n",
      "\n",
      "Epoch 00042: val_loss did not improve.\n",
      "Epoch 43/45\n",
      " - 26s - loss: 0.0095 - val_loss: 0.0094\n",
      "\n",
      "Epoch 00043: val_loss improved from 0.00945 to 0.00943, storing weights.\n",
      "Epoch 44/45\n",
      " - 25s - loss: 0.0093 - val_loss: 0.0094\n",
      "\n",
      "Epoch 00044: val_loss improved from 0.00943 to 0.00942, storing weights.\n",
      "Epoch 45/45\n",
      " - 25s - loss: 0.0092 - val_loss: 0.0094\n",
      "\n",
      "Epoch 00045: val_loss improved from 0.00942 to 0.00939, storing weights.\n",
      "Using epoch 00045 with val_loss: 0.00939.\n"
     ]
    }
   ],
   "source": [
    "param = {'model_type':'esp'\n",
    "'em_drop':0.2,\n",
    "'rnn_drop':0.5,\n",
    "'rnn_rec_drop':0.4,\n",
    "'fc_drop':0.4,\n",
    "'batch_size':80,\n",
    "'epochs':45,\n",
    "'em_dim':44,\n",
    "'rnn_units':80,\n",
    "'fc_num_hidden_layers':3,\n",
    "'fc_num_units':300,\n",
    "'fc_activation':'3',\n",
    "'optimizer':'6'}\n",
    "\n",
    "model = lstm_model(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('MES:0.007463059430260549', 'Spearman:0.8923760125364569')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_metrics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.preprocessing import text\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers import merge, Embedding, Bidirectional\n",
    "from keras.layers.core import *\n",
    "from keras.models import *\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(model_type='esp', batch_size=90, epochs=50, initializer='0',em_dim=44,em_drop=0.2,\n",
    "                rnn_units=60, rnn_drop=0.6, rnn_rec_drop=0.1, fc_num_hidden_layers=3,\n",
    "                fc_num_units=320, fc_drop=0.4,fc_activation='elu',optimizer=Adam,learning_rate=0.001,\n",
    "                validation_split=0.1,shuffle=False):\n",
    "    if model_type == 'esp':\n",
    "        X,X_biofeat,y = esp_data\n",
    "    elif model_type == 'hf':\n",
    "        X,X_biofeat,y = hf_data\n",
    "    X_train, X_test, X_train_biofeat, X_test_biofeat, y_train, y_test = load_data(X, X_biofeat, y,random_state=33) \n",
    "    \n",
    "    fc_activation = fc_activation_dict[str(fc_activation)]\n",
    "    initializer = initializer_dict[str(initializer)]\n",
    "    optimizer = optimizer_dict[str(optimizer)]\n",
    "    sequence_input = Input(name = 'seq_input', shape = (22,))\n",
    "\n",
    "    embedding_layer = Embedding(7,em_dim,input_length=22)\n",
    "    embedded = embedding_layer(sequence_input)\n",
    "    embedded = SpatialDropout1D(em_drop)(embedded)\n",
    "    x = embedded\n",
    "\n",
    "    #RNN\n",
    "    lstm = LSTM(rnn_units, dropout=rnn_drop, \n",
    "                kernel_regularizer='l2',recurrent_regularizer='l2',\n",
    "                recurrent_dropout=rnn_rec_drop, return_sequences=True)\n",
    "    x = Bidirectional(lstm)(x)\n",
    "    x = Flatten()(x)\n",
    "\n",
    "#     #生物学特征\n",
    "    biological_input = Input(name = 'bio_input', shape = (X_train_biofeat.shape[1],))\n",
    "    x = keras.layers.concatenate([x, biological_input])\n",
    "\n",
    "\n",
    "    for l in range(fc_num_hidden_layers):\n",
    "        x = Dense(fc_num_units, activation=fc_activation)(x)\n",
    "        x = Dropout(fc_drop)(x)\n",
    "    #finish model\n",
    "    mix_output = Dense(1, activation='linear',name='mix_output')(x)\n",
    "\n",
    "    model = Model(inputs=[sequence_input, biological_input], outputs=[mix_output])\n",
    "    #model = Model(inputs=[sequence_input], outputs=[mix_output])\n",
    "    model.compile(loss='mse', optimizer=optimizer(lr=0.001))\n",
    "    \n",
    "    np.random.seed(1337)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "    get_best_model = GetBest('Models/esp_rnn.hd5',monitor='val_loss', verbose=1, mode='min')\n",
    "    model.fit([X_train,X_train_biofeat], \n",
    "    #model.fit([X_train], \n",
    "                 y_train,\n",
    "                 batch_size=batch_size,\n",
    "                 epochs=epochs,\n",
    "                 verbose=2,\n",
    "                 validation_split=0.1,\n",
    "                 shuffle=False,\n",
    "                 callbacks=[get_best_model, early_stopping])    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc_activation_dict = {'1':'relu','2':'tanh', '3':'sigmoid', '4':'hard_sigmoid', '0':'elu'}\n",
    "initializer_dict = {'1':'lecun_uniform','2':'normal', '3':'he_normal', '0':'he_uniform'}\n",
    "optimizer_dict = {'1':SGD,'2':RMSprop, '3':Adagrad, '4':Adadelta,'5':Adam,'6':Adamax,'0':Nadam}\n",
    "\n",
    "import numpy as np\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "            \n",
    "class GetBest(Callback):\n",
    "    def __init__(self,filepath=None, monitor='val_loss', save_best=False,verbose=0,\n",
    "                 mode='auto', period=1):\n",
    "        super(GetBest, self).__init__()\n",
    "        self.monitor = monitor\n",
    "        self.verbose = verbose\n",
    "        self.period = period\n",
    "        self.save_best = save_best\n",
    "        self.filepath = filepath\n",
    "        self.best_epochs = 0\n",
    "        self.epochs_since_last_save = 0\n",
    "\n",
    "        if mode not in ['auto', 'min', 'max']:\n",
    "            warnings.warn('GetBest mode %s is unknown, '\n",
    "                          'fallback to auto mode.' % (mode),\n",
    "                          RuntimeWarning)\n",
    "            mode = 'auto'\n",
    "\n",
    "        if mode == 'min':\n",
    "            self.monitor_op = np.less\n",
    "            self.best = np.Inf\n",
    "        elif mode == 'max':\n",
    "            self.monitor_op = np.greater\n",
    "            self.best = -np.Inf\n",
    "        else:\n",
    "            if 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
    "                self.monitor_op = np.greater\n",
    "                self.best = -np.Inf\n",
    "            else:\n",
    "                self.monitor_op = np.less\n",
    "                self.best = np.Inf\n",
    "                \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.best_weights = self.model.get_weights()\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        self.epochs_since_last_save += 1\n",
    "        if self.epochs_since_last_save >= self.period:\n",
    "            self.epochs_since_last_save = 0\n",
    "            filepath = self.filepath.format(epoch=epoch + 1, **logs)\n",
    "            current = logs.get(self.monitor)\n",
    "            if current is None:\n",
    "                warnings.warn('Can pick best model only with %s available, '\n",
    "                              'skipping.' % (self.monitor), RuntimeWarning)\n",
    "            else:\n",
    "                if self.monitor_op(current, self.best):\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s improved from %0.5f to %0.5f,'\n",
    "                              ' storing weights.'\n",
    "                              % (epoch + 1, self.monitor, self.best,\n",
    "                                 current))\n",
    "                    self.best = current\n",
    "                    self.best_epochs = epoch + 1\n",
    "                    self.best_weights = self.model.get_weights()\n",
    "                    self.model.save(filepath, overwrite=True)\n",
    "                else:\n",
    "                    if self.verbose > 0:\n",
    "                        print('\\nEpoch %05d: %s did not improve.' %\n",
    "                              (epoch + 1, self.monitor)) \n",
    "                    \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.verbose > 0:\n",
    "            print('Using epoch %05d with %s: %0.5f.' % (self.best_epochs, self.monitor,\n",
    "                                                       self.best))\n",
    "        self.model.set_weights(self.best_weights)\n",
    "        self.model.save(self.filepath, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(crispr)",
   "language": "python",
   "name": "crispr"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
